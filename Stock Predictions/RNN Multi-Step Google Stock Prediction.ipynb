{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.01\n",
    "hidden_size_lstm = 32\n",
    "hidden_layers_lstm = 2\n",
    "test_size = 1\n",
    "T = 30\n",
    "PredictionSteps = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19</th>\n",
       "      <td>50.050049</td>\n",
       "      <td>52.082081</td>\n",
       "      <td>48.028027</td>\n",
       "      <td>50.220219</td>\n",
       "      <td>50.220219</td>\n",
       "      <td>44659096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20</th>\n",
       "      <td>50.555557</td>\n",
       "      <td>54.594597</td>\n",
       "      <td>50.300301</td>\n",
       "      <td>54.209209</td>\n",
       "      <td>54.209209</td>\n",
       "      <td>22834343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23</th>\n",
       "      <td>55.430431</td>\n",
       "      <td>56.796799</td>\n",
       "      <td>54.579578</td>\n",
       "      <td>54.754753</td>\n",
       "      <td>54.754753</td>\n",
       "      <td>18256126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24</th>\n",
       "      <td>55.675674</td>\n",
       "      <td>55.855858</td>\n",
       "      <td>51.836838</td>\n",
       "      <td>52.487488</td>\n",
       "      <td>52.487488</td>\n",
       "      <td>15247337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25</th>\n",
       "      <td>52.532532</td>\n",
       "      <td>54.054054</td>\n",
       "      <td>51.991993</td>\n",
       "      <td>53.053055</td>\n",
       "      <td>53.053055</td>\n",
       "      <td>9188602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume\n",
       "Date                                                                       \n",
       "2004-08-19  50.050049  52.082081  48.028027  50.220219  50.220219  44659096\n",
       "2004-08-20  50.555557  54.594597  50.300301  54.209209  54.209209  22834343\n",
       "2004-08-23  55.430431  56.796799  54.579578  54.754753  54.754753  18256126\n",
       "2004-08-24  55.675674  55.855858  51.836838  52.487488  52.487488  15247337\n",
       "2004-08-25  52.532532  54.054054  51.991993  53.053055  53.053055   9188602"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./../data/GOOGL.csv\", header=0, index_col=0)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: (4495, 6)\n",
      "Outputs shape: (4495, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :]\n",
    "# X = np.reshape(np.array(df.iloc[:, 0]), (-1, 1))\n",
    "\n",
    "Y = np.reshape(np.array(df.iloc[:, 0]), (-1, 1))\n",
    "\n",
    "print(f\"Inputs shape: {X.shape}\")\n",
    "print(f\"Outputs shape: {Y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4450, Test size: 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "N_train = np.int64(X.shape[0] * test_size)\n",
    "N_tot = X.shape[0]\n",
    "\n",
    "if (N_tot-N_train < PredictionSteps+T): N_train=N_tot-(PredictionSteps+T+1)\n",
    "print(f'Train size: {N_train}, Test size: {N_tot-N_train}')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# we scale by looking only on the training data, thus test data is not considered\n",
    "scaler.fit(X[:N_train])\n",
    "# but we still apply scaling on all the data since we need to be able to make predictions\n",
    "X = scaler.transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# we scale by looking only on the training data, thus test data is not considered\n",
    "scaler.fit(Y[:N_train])\n",
    "# but we still apply scaling on all the data since we need to be able to make predictions\n",
    "Y = scaler.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.zeros((N_tot - T - PredictionSteps, T, X.shape[1]))\n",
    "targets = np.zeros((N_tot - T - PredictionSteps, PredictionSteps))\n",
    "\n",
    "for t in range(N_tot - T - PredictionSteps):\n",
    "    inputs[t] = X[t : t + T]\n",
    "    targets[t] = np.reshape(Y[t + T : t + T + PredictionSteps], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: torch.Size([4450, 30, 6]) for Inputs and torch.Size([4450, 14]) for Targets\n",
      "Test shapes: torch.Size([1, 30, 6]) for Inputs and torch.Size([1, 14]) for Targets\n"
     ]
    }
   ],
   "source": [
    "train_inputs = torch.from_numpy(inputs[:N_train].astype(np.float32)).to(device)\n",
    "train_targets = torch.from_numpy(targets[:N_train].astype(np.float32)).to(device)\n",
    "test_inputs = torch.from_numpy(inputs[N_train:].astype(np.float32)).to(device)\n",
    "test_targets = torch.from_numpy(targets[N_train:].astype(np.float32)).to(device)\n",
    "\n",
    "print(\n",
    "    f\"Train shapes: {train_inputs.shape} for Inputs and {train_targets.shape} for Targets\"\n",
    ")\n",
    "print(\n",
    "    f\"Test shapes: {test_inputs.shape} for Inputs and {test_targets.shape} for Targets\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, outputs_size, device=\"cpu\"):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.D = input_size\n",
    "        self.M = hidden_size\n",
    "        self.L = num_layers\n",
    "        self.K = outputs_size\n",
    "\n",
    "        self.rnn_layer = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.M, self.K)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        h0 = torch.zeros((self.L, inputs.size(0), self.M)).to(self.device)\n",
    "        c0 = torch.zeros((self.L, inputs.size(0), self.M)).to(self.device)\n",
    "\n",
    "        outputs, _ = self.rnn_layer(inputs, (h0, c0))\n",
    "        outputs = self.fc(outputs[:, -1, :])\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn_layer): LSTM(6, 32, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(\n",
    "    input_size=inputs.shape[2],\n",
    "    outputs_size=targets.shape[1],\n",
    "    hidden_size=hidden_size_lstm,\n",
    "    num_layers=hidden_layers_lstm,\n",
    "    device=device,\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    test_inputs,\n",
    "    test_targets,\n",
    "    n_epochs=200,\n",
    "):\n",
    "    train_losses = np.zeros(n_epochs)\n",
    "    test_losses = np.zeros(n_epochs)\n",
    "\n",
    "    for it in range(n_epochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_inputs)\n",
    "        loss = criterion(outputs, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses[it] = loss.item()\n",
    "        test_losses[it] = criterion(model(test_inputs), test_targets).item()\n",
    "\n",
    "        print(\n",
    "            f\"Iteration: {it+1:2.0f}/{n_epochs} \\t Train Loss: {train_losses[it]:.4f} \\t Test Loss: {test_losses[it]:.4f}\"\n",
    "        )\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1/1000 \t Train Loss: 1.0549 \t Test Loss: 5.2931\n",
      "Iteration:  2/1000 \t Train Loss: 0.9984 \t Test Loss: 4.9363\n",
      "Iteration:  3/1000 \t Train Loss: 0.9169 \t Test Loss: 4.4311\n",
      "Iteration:  4/1000 \t Train Loss: 0.7824 \t Test Loss: 3.8123\n",
      "Iteration:  5/1000 \t Train Loss: 0.6281 \t Test Loss: 3.0978\n",
      "Iteration:  6/1000 \t Train Loss: 0.5151 \t Test Loss: 2.3826\n",
      "Iteration:  7/1000 \t Train Loss: 0.4404 \t Test Loss: 1.7483\n",
      "Iteration:  8/1000 \t Train Loss: 0.3975 \t Test Loss: 1.2356\n",
      "Iteration:  9/1000 \t Train Loss: 0.3708 \t Test Loss: 0.8764\n",
      "Iteration: 10/1000 \t Train Loss: 0.3442 \t Test Loss: 0.6581\n",
      "Iteration: 11/1000 \t Train Loss: 0.3009 \t Test Loss: 0.5575\n",
      "Iteration: 12/1000 \t Train Loss: 0.2421 \t Test Loss: 0.5592\n",
      "Iteration: 13/1000 \t Train Loss: 0.1952 \t Test Loss: 0.5561\n",
      "Iteration: 14/1000 \t Train Loss: 0.1633 \t Test Loss: 0.4783\n",
      "Iteration: 15/1000 \t Train Loss: 0.1336 \t Test Loss: 0.2928\n",
      "Iteration: 16/1000 \t Train Loss: 0.0961 \t Test Loss: 0.1261\n",
      "Iteration: 17/1000 \t Train Loss: 0.0632 \t Test Loss: 0.1026\n",
      "Iteration: 18/1000 \t Train Loss: 0.0431 \t Test Loss: 0.1741\n",
      "Iteration: 19/1000 \t Train Loss: 0.0349 \t Test Loss: 0.2866\n",
      "Iteration: 20/1000 \t Train Loss: 0.0337 \t Test Loss: 0.3475\n",
      "Iteration: 21/1000 \t Train Loss: 0.0305 \t Test Loss: 0.2312\n",
      "Iteration: 22/1000 \t Train Loss: 0.0227 \t Test Loss: 0.0995\n",
      "Iteration: 23/1000 \t Train Loss: 0.0208 \t Test Loss: 0.0653\n",
      "Iteration: 24/1000 \t Train Loss: 0.0201 \t Test Loss: 0.0674\n",
      "Iteration: 25/1000 \t Train Loss: 0.0148 \t Test Loss: 0.1260\n",
      "Iteration: 26/1000 \t Train Loss: 0.0150 \t Test Loss: 0.1399\n",
      "Iteration: 27/1000 \t Train Loss: 0.0161 \t Test Loss: 0.0526\n",
      "Iteration: 28/1000 \t Train Loss: 0.0128 \t Test Loss: 0.0241\n",
      "Iteration: 29/1000 \t Train Loss: 0.0150 \t Test Loss: 0.0235\n",
      "Iteration: 30/1000 \t Train Loss: 0.0120 \t Test Loss: 0.0545\n",
      "Iteration: 31/1000 \t Train Loss: 0.0119 \t Test Loss: 0.0383\n",
      "Iteration: 32/1000 \t Train Loss: 0.0104 \t Test Loss: 0.0214\n",
      "Iteration: 33/1000 \t Train Loss: 0.0099 \t Test Loss: 0.0229\n",
      "Iteration: 34/1000 \t Train Loss: 0.0090 \t Test Loss: 0.0523\n",
      "Iteration: 35/1000 \t Train Loss: 0.0087 \t Test Loss: 0.0579\n",
      "Iteration: 36/1000 \t Train Loss: 0.0082 \t Test Loss: 0.0299\n",
      "Iteration: 37/1000 \t Train Loss: 0.0076 \t Test Loss: 0.0286\n",
      "Iteration: 38/1000 \t Train Loss: 0.0076 \t Test Loss: 0.0633\n",
      "Iteration: 39/1000 \t Train Loss: 0.0072 \t Test Loss: 0.0799\n",
      "Iteration: 40/1000 \t Train Loss: 0.0071 \t Test Loss: 0.0446\n",
      "Iteration: 41/1000 \t Train Loss: 0.0065 \t Test Loss: 0.0306\n",
      "Iteration: 42/1000 \t Train Loss: 0.0066 \t Test Loss: 0.0486\n",
      "Iteration: 43/1000 \t Train Loss: 0.0062 \t Test Loss: 0.0621\n",
      "Iteration: 44/1000 \t Train Loss: 0.0065 \t Test Loss: 0.0374\n",
      "Iteration: 45/1000 \t Train Loss: 0.0061 \t Test Loss: 0.0264\n",
      "Iteration: 46/1000 \t Train Loss: 0.0062 \t Test Loss: 0.0420\n",
      "Iteration: 47/1000 \t Train Loss: 0.0059 \t Test Loss: 0.0539\n",
      "Iteration: 48/1000 \t Train Loss: 0.0060 \t Test Loss: 0.0338\n",
      "Iteration: 49/1000 \t Train Loss: 0.0057 \t Test Loss: 0.0298\n",
      "Iteration: 50/1000 \t Train Loss: 0.0056 \t Test Loss: 0.0497\n",
      "Iteration: 51/1000 \t Train Loss: 0.0053 \t Test Loss: 0.0558\n",
      "Iteration: 52/1000 \t Train Loss: 0.0053 \t Test Loss: 0.0393\n",
      "Iteration: 53/1000 \t Train Loss: 0.0052 \t Test Loss: 0.0403\n",
      "Iteration: 54/1000 \t Train Loss: 0.0051 \t Test Loss: 0.0595\n",
      "Iteration: 55/1000 \t Train Loss: 0.0050 \t Test Loss: 0.0603\n",
      "Iteration: 56/1000 \t Train Loss: 0.0050 \t Test Loss: 0.0433\n",
      "Iteration: 57/1000 \t Train Loss: 0.0049 \t Test Loss: 0.0432\n",
      "Iteration: 58/1000 \t Train Loss: 0.0048 \t Test Loss: 0.0544\n",
      "Iteration: 59/1000 \t Train Loss: 0.0048 \t Test Loss: 0.0474\n",
      "Iteration: 60/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0350\n",
      "Iteration: 61/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0397\n",
      "Iteration: 62/1000 \t Train Loss: 0.0046 \t Test Loss: 0.0471\n",
      "Iteration: 63/1000 \t Train Loss: 0.0046 \t Test Loss: 0.0388\n",
      "Iteration: 64/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0358\n",
      "Iteration: 65/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0456\n",
      "Iteration: 66/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0475\n",
      "Iteration: 67/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0401\n",
      "Iteration: 68/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0427\n",
      "Iteration: 69/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0511\n",
      "Iteration: 70/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0486\n",
      "Iteration: 71/1000 \t Train Loss: 0.0043 \t Test Loss: 0.0433\n",
      "Iteration: 72/1000 \t Train Loss: 0.0043 \t Test Loss: 0.0477\n",
      "Iteration: 73/1000 \t Train Loss: 0.0043 \t Test Loss: 0.0519\n",
      "Iteration: 74/1000 \t Train Loss: 0.0043 \t Test Loss: 0.0458\n",
      "Iteration: 75/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0424\n",
      "Iteration: 76/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0470\n",
      "Iteration: 77/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0474\n",
      "Iteration: 78/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0425\n",
      "Iteration: 79/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0441\n",
      "Iteration: 80/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0490\n",
      "Iteration: 81/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0469\n",
      "Iteration: 82/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0444\n",
      "Iteration: 83/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0480\n",
      "Iteration: 84/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0493\n",
      "Iteration: 85/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0456\n",
      "Iteration: 86/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0459\n",
      "Iteration: 87/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0489\n",
      "Iteration: 88/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0467\n",
      "Iteration: 89/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0442\n",
      "Iteration: 90/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0462\n",
      "Iteration: 91/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0460\n",
      "Iteration: 92/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0428\n",
      "Iteration: 93/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0436\n",
      "Iteration: 94/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0453\n",
      "Iteration: 95/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0433\n",
      "Iteration: 96/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0431\n",
      "Iteration: 97/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0454\n",
      "Iteration: 98/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0443\n",
      "Iteration: 99/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0432\n",
      "Iteration: 100/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0451\n",
      "Iteration: 101/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0451\n",
      "Iteration: 102/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0436\n",
      "Iteration: 103/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0450\n",
      "Iteration: 104/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0455\n",
      "Iteration: 105/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0439\n",
      "Iteration: 106/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0445\n",
      "Iteration: 107/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0453\n",
      "Iteration: 108/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0440\n",
      "Iteration: 109/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0443\n",
      "Iteration: 110/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0454\n",
      "Iteration: 111/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0444\n",
      "Iteration: 112/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0446\n",
      "Iteration: 113/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0456\n",
      "Iteration: 114/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0447\n",
      "Iteration: 115/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0446\n",
      "Iteration: 116/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0454\n",
      "Iteration: 117/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0446\n",
      "Iteration: 118/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0443\n",
      "Iteration: 119/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0449\n",
      "Iteration: 120/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0440\n",
      "Iteration: 121/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0438\n",
      "Iteration: 122/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0443\n",
      "Iteration: 123/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0434\n",
      "Iteration: 124/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0435\n",
      "Iteration: 125/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0440\n",
      "Iteration: 126/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0433\n",
      "Iteration: 127/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0436\n",
      "Iteration: 128/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0438\n",
      "Iteration: 129/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0433\n",
      "Iteration: 130/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0438\n",
      "Iteration: 131/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0435\n",
      "Iteration: 132/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0432\n",
      "Iteration: 133/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0436\n",
      "Iteration: 134/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0431\n",
      "Iteration: 135/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0432\n",
      "Iteration: 136/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0432\n",
      "Iteration: 137/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0428\n",
      "Iteration: 138/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0431\n",
      "Iteration: 139/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0428\n",
      "Iteration: 140/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0428\n",
      "Iteration: 141/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0429\n",
      "Iteration: 142/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0425\n",
      "Iteration: 143/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0428\n",
      "Iteration: 144/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0423\n",
      "Iteration: 145/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0425\n",
      "Iteration: 146/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0422\n",
      "Iteration: 147/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0421\n",
      "Iteration: 148/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0421\n",
      "Iteration: 149/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0418\n",
      "Iteration: 150/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0420\n",
      "Iteration: 151/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0415\n",
      "Iteration: 152/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0419\n",
      "Iteration: 153/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0412\n",
      "Iteration: 154/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0420\n",
      "Iteration: 155/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0406\n",
      "Iteration: 156/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0424\n",
      "Iteration: 157/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0395\n",
      "Iteration: 158/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0440\n",
      "Iteration: 159/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0363\n",
      "Iteration: 160/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0503\n",
      "Iteration: 161/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0283\n",
      "Iteration: 162/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0760\n",
      "Iteration: 163/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0194\n",
      "Iteration: 164/1000 \t Train Loss: 0.0051 \t Test Loss: 0.0897\n",
      "Iteration: 165/1000 \t Train Loss: 0.0051 \t Test Loss: 0.0399\n",
      "Iteration: 166/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0252\n",
      "Iteration: 167/1000 \t Train Loss: 0.0046 \t Test Loss: 0.0713\n",
      "Iteration: 168/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0599\n",
      "Iteration: 169/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0272\n",
      "Iteration: 170/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0461\n",
      "Iteration: 171/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0706\n",
      "Iteration: 172/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0367\n",
      "Iteration: 173/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0288\n",
      "Iteration: 174/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0544\n",
      "Iteration: 175/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0488\n",
      "Iteration: 176/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0270\n",
      "Iteration: 177/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0377\n",
      "Iteration: 178/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0545\n",
      "Iteration: 179/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0345\n",
      "Iteration: 180/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0288\n",
      "Iteration: 181/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0463\n",
      "Iteration: 182/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0450\n",
      "Iteration: 183/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0303\n",
      "Iteration: 184/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0367\n",
      "Iteration: 185/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0496\n",
      "Iteration: 186/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0390\n",
      "Iteration: 187/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0322\n",
      "Iteration: 188/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0430\n",
      "Iteration: 189/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0468\n",
      "Iteration: 190/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0355\n",
      "Iteration: 191/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0368\n",
      "Iteration: 192/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0474\n",
      "Iteration: 193/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0427\n",
      "Iteration: 194/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0354\n",
      "Iteration: 195/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0412\n",
      "Iteration: 196/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0461\n",
      "Iteration: 197/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0388\n",
      "Iteration: 198/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0372\n",
      "Iteration: 199/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0444\n",
      "Iteration: 200/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0435\n",
      "Iteration: 201/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0371\n",
      "Iteration: 202/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0401\n",
      "Iteration: 203/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0448\n",
      "Iteration: 204/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0401\n",
      "Iteration: 205/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0382\n",
      "Iteration: 206/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0433\n",
      "Iteration: 207/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0431\n",
      "Iteration: 208/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0386\n",
      "Iteration: 209/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0403\n",
      "Iteration: 210/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0435\n",
      "Iteration: 211/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0403\n",
      "Iteration: 212/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0387\n",
      "Iteration: 213/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0423\n",
      "Iteration: 214/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0418\n",
      "Iteration: 215/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0386\n",
      "Iteration: 216/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0404\n",
      "Iteration: 217/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0422\n",
      "Iteration: 218/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0397\n",
      "Iteration: 219/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0395\n",
      "Iteration: 220/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0421\n",
      "Iteration: 221/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0411\n",
      "Iteration: 222/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0393\n",
      "Iteration: 223/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0412\n",
      "Iteration: 224/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0417\n",
      "Iteration: 225/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0396\n",
      "Iteration: 226/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0404\n",
      "Iteration: 227/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0417\n",
      "Iteration: 228/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0401\n",
      "Iteration: 229/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0397\n",
      "Iteration: 230/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0411\n",
      "Iteration: 231/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0404\n",
      "Iteration: 232/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0393\n",
      "Iteration: 233/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0405\n",
      "Iteration: 234/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0405\n",
      "Iteration: 235/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0392\n",
      "Iteration: 236/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0399\n",
      "Iteration: 237/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0404\n",
      "Iteration: 238/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0392\n",
      "Iteration: 239/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0395\n",
      "Iteration: 240/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0402\n",
      "Iteration: 241/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0393\n",
      "Iteration: 242/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0392\n",
      "Iteration: 243/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0399\n",
      "Iteration: 244/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0393\n",
      "Iteration: 245/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0389\n",
      "Iteration: 246/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0396\n",
      "Iteration: 247/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0392\n",
      "Iteration: 248/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0388\n",
      "Iteration: 249/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0393\n",
      "Iteration: 250/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0390\n",
      "Iteration: 251/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0386\n",
      "Iteration: 252/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0390\n",
      "Iteration: 253/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0388\n",
      "Iteration: 254/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0383\n",
      "Iteration: 255/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0387\n",
      "Iteration: 256/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0385\n",
      "Iteration: 257/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0381\n",
      "Iteration: 258/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0384\n",
      "Iteration: 259/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0381\n",
      "Iteration: 260/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0378\n",
      "Iteration: 261/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0381\n",
      "Iteration: 262/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0378\n",
      "Iteration: 263/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0375\n",
      "Iteration: 264/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0377\n",
      "Iteration: 265/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 266/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0372\n",
      "Iteration: 267/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 268/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0370\n",
      "Iteration: 269/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0370\n",
      "Iteration: 270/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0370\n",
      "Iteration: 271/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0366\n",
      "Iteration: 272/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0367\n",
      "Iteration: 273/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0365\n",
      "Iteration: 274/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0363\n",
      "Iteration: 275/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0363\n",
      "Iteration: 276/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0361\n",
      "Iteration: 277/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 278/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 279/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 280/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 281/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 282/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 283/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 284/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 285/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 286/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 287/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 288/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 289/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0344\n",
      "Iteration: 290/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0344\n",
      "Iteration: 291/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0342\n",
      "Iteration: 292/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0341\n",
      "Iteration: 293/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0340\n",
      "Iteration: 294/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0339\n",
      "Iteration: 295/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0338\n",
      "Iteration: 296/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0335\n",
      "Iteration: 297/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0337\n",
      "Iteration: 298/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0331\n",
      "Iteration: 299/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0338\n",
      "Iteration: 300/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0324\n",
      "Iteration: 301/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 302/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0302\n",
      "Iteration: 303/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0396\n",
      "Iteration: 304/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0234\n",
      "Iteration: 305/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0724\n",
      "Iteration: 306/1000 \t Train Loss: 0.0048 \t Test Loss: 0.0191\n",
      "Iteration: 307/1000 \t Train Loss: 0.0066 \t Test Loss: 0.1658\n",
      "Iteration: 308/1000 \t Train Loss: 0.0096 \t Test Loss: 0.0223\n",
      "Iteration: 309/1000 \t Train Loss: 0.0063 \t Test Loss: 0.0205\n",
      "Iteration: 310/1000 \t Train Loss: 0.0060 \t Test Loss: 0.1073\n",
      "Iteration: 311/1000 \t Train Loss: 0.0064 \t Test Loss: 0.0560\n",
      "Iteration: 312/1000 \t Train Loss: 0.0049 \t Test Loss: 0.0215\n",
      "Iteration: 313/1000 \t Train Loss: 0.0066 \t Test Loss: 0.0885\n",
      "Iteration: 314/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0954\n",
      "Iteration: 315/1000 \t Train Loss: 0.0053 \t Test Loss: 0.0342\n",
      "Iteration: 316/1000 \t Train Loss: 0.0046 \t Test Loss: 0.0344\n",
      "Iteration: 317/1000 \t Train Loss: 0.0049 \t Test Loss: 0.0789\n",
      "Iteration: 318/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0723\n",
      "Iteration: 319/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0339\n",
      "Iteration: 320/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0381\n",
      "Iteration: 321/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0705\n",
      "Iteration: 322/1000 \t Train Loss: 0.0046 \t Test Loss: 0.0549\n",
      "Iteration: 323/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0262\n",
      "Iteration: 324/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0308\n",
      "Iteration: 325/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0573\n",
      "Iteration: 326/1000 \t Train Loss: 0.0043 \t Test Loss: 0.0476\n",
      "Iteration: 327/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0273\n",
      "Iteration: 328/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0340\n",
      "Iteration: 329/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0545\n",
      "Iteration: 330/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0414\n",
      "Iteration: 331/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0262\n",
      "Iteration: 332/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0345\n",
      "Iteration: 333/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0527\n",
      "Iteration: 334/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0421\n",
      "Iteration: 335/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0301\n",
      "Iteration: 336/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0379\n",
      "Iteration: 337/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0500\n",
      "Iteration: 338/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0404\n",
      "Iteration: 339/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0314\n",
      "Iteration: 340/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0377\n",
      "Iteration: 341/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0481\n",
      "Iteration: 342/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0429\n",
      "Iteration: 343/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0347\n",
      "Iteration: 344/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0378\n",
      "Iteration: 345/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0456\n",
      "Iteration: 346/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0426\n",
      "Iteration: 347/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0355\n",
      "Iteration: 348/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0375\n",
      "Iteration: 349/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0456\n",
      "Iteration: 350/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0445\n",
      "Iteration: 351/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0372\n",
      "Iteration: 352/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0376\n",
      "Iteration: 353/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0436\n",
      "Iteration: 354/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0429\n",
      "Iteration: 355/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0373\n",
      "Iteration: 356/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0377\n",
      "Iteration: 357/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0427\n",
      "Iteration: 358/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0426\n",
      "Iteration: 359/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0380\n",
      "Iteration: 360/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0372\n",
      "Iteration: 361/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0404\n",
      "Iteration: 362/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0410\n",
      "Iteration: 363/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0377\n",
      "Iteration: 364/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0368\n",
      "Iteration: 365/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0397\n",
      "Iteration: 366/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0406\n",
      "Iteration: 367/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0375\n",
      "Iteration: 368/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0365\n",
      "Iteration: 369/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0390\n",
      "Iteration: 370/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0399\n",
      "Iteration: 371/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 372/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 373/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0394\n",
      "Iteration: 374/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0401\n",
      "Iteration: 375/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0383\n",
      "Iteration: 376/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0376\n",
      "Iteration: 377/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0391\n",
      "Iteration: 378/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0399\n",
      "Iteration: 379/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0386\n",
      "Iteration: 380/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 381/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0390\n",
      "Iteration: 382/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0394\n",
      "Iteration: 383/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0381\n",
      "Iteration: 384/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 385/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0384\n",
      "Iteration: 386/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0388\n",
      "Iteration: 387/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 388/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 389/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0382\n",
      "Iteration: 390/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0384\n",
      "Iteration: 391/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0376\n",
      "Iteration: 392/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 393/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0380\n",
      "Iteration: 394/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0383\n",
      "Iteration: 395/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0377\n",
      "Iteration: 396/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 397/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 398/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0380\n",
      "Iteration: 399/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0375\n",
      "Iteration: 400/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 401/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 402/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 403/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 404/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0373\n",
      "Iteration: 405/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0376\n",
      "Iteration: 406/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0376\n",
      "Iteration: 407/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0372\n",
      "Iteration: 408/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0372\n",
      "Iteration: 409/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0375\n",
      "Iteration: 410/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0374\n",
      "Iteration: 411/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0370\n",
      "Iteration: 412/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0370\n",
      "Iteration: 413/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0373\n",
      "Iteration: 414/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0371\n",
      "Iteration: 415/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0369\n",
      "Iteration: 416/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0370\n",
      "Iteration: 417/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0371\n",
      "Iteration: 418/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0369\n",
      "Iteration: 419/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0367\n",
      "Iteration: 420/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0368\n",
      "Iteration: 421/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0369\n",
      "Iteration: 422/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0367\n",
      "Iteration: 423/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0366\n",
      "Iteration: 424/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0367\n",
      "Iteration: 425/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0367\n",
      "Iteration: 426/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0365\n",
      "Iteration: 427/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0365\n",
      "Iteration: 428/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0366\n",
      "Iteration: 429/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0366\n",
      "Iteration: 430/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0364\n",
      "Iteration: 431/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0364\n",
      "Iteration: 432/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0365\n",
      "Iteration: 433/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0364\n",
      "Iteration: 434/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0363\n",
      "Iteration: 435/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0363\n",
      "Iteration: 436/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0363\n",
      "Iteration: 437/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0362\n",
      "Iteration: 438/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0362\n",
      "Iteration: 439/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0362\n",
      "Iteration: 440/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0362\n",
      "Iteration: 441/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0361\n",
      "Iteration: 442/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0360\n",
      "Iteration: 443/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0360\n",
      "Iteration: 444/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0360\n",
      "Iteration: 445/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 446/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 447/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 448/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0358\n",
      "Iteration: 449/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0358\n",
      "Iteration: 450/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0358\n",
      "Iteration: 451/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0358\n",
      "Iteration: 452/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 453/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 454/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 455/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 456/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 457/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 458/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 459/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 460/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 461/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 462/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 463/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 464/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 465/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 466/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 467/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 468/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 469/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 470/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 471/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 472/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 473/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 474/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 475/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 476/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 477/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 478/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 479/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 480/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 481/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 482/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 483/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 484/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 485/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 486/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 487/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 488/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0346\n",
      "Iteration: 489/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0346\n",
      "Iteration: 490/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0346\n",
      "Iteration: 491/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0345\n",
      "Iteration: 492/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0345\n",
      "Iteration: 493/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0345\n",
      "Iteration: 494/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0344\n",
      "Iteration: 495/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0344\n",
      "Iteration: 496/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0344\n",
      "Iteration: 497/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0343\n",
      "Iteration: 498/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0343\n",
      "Iteration: 499/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0343\n",
      "Iteration: 500/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0342\n",
      "Iteration: 501/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0342\n",
      "Iteration: 502/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0342\n",
      "Iteration: 503/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0341\n",
      "Iteration: 504/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0341\n",
      "Iteration: 505/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0340\n",
      "Iteration: 506/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0340\n",
      "Iteration: 507/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0340\n",
      "Iteration: 508/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0339\n",
      "Iteration: 509/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0339\n",
      "Iteration: 510/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0339\n",
      "Iteration: 511/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0338\n",
      "Iteration: 512/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0338\n",
      "Iteration: 513/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0337\n",
      "Iteration: 514/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0337\n",
      "Iteration: 515/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0337\n",
      "Iteration: 516/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0336\n",
      "Iteration: 517/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0336\n",
      "Iteration: 518/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0335\n",
      "Iteration: 519/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0335\n",
      "Iteration: 520/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0334\n",
      "Iteration: 521/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0334\n",
      "Iteration: 522/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0334\n",
      "Iteration: 523/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0333\n",
      "Iteration: 524/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0333\n",
      "Iteration: 525/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0332\n",
      "Iteration: 526/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0332\n",
      "Iteration: 527/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0331\n",
      "Iteration: 528/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0331\n",
      "Iteration: 529/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0330\n",
      "Iteration: 530/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0330\n",
      "Iteration: 531/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0329\n",
      "Iteration: 532/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0329\n",
      "Iteration: 533/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0328\n",
      "Iteration: 534/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0327\n",
      "Iteration: 535/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0327\n",
      "Iteration: 536/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0326\n",
      "Iteration: 537/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0326\n",
      "Iteration: 538/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0325\n",
      "Iteration: 539/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0324\n",
      "Iteration: 540/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0324\n",
      "Iteration: 541/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0323\n",
      "Iteration: 542/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0322\n",
      "Iteration: 543/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0322\n",
      "Iteration: 544/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0321\n",
      "Iteration: 545/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0320\n",
      "Iteration: 546/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0320\n",
      "Iteration: 547/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0319\n",
      "Iteration: 548/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0318\n",
      "Iteration: 549/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0317\n",
      "Iteration: 550/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0317\n",
      "Iteration: 551/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0316\n",
      "Iteration: 552/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0315\n",
      "Iteration: 553/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0314\n",
      "Iteration: 554/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0313\n",
      "Iteration: 555/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0312\n",
      "Iteration: 556/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0311\n",
      "Iteration: 557/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0311\n",
      "Iteration: 558/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0310\n",
      "Iteration: 559/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0309\n",
      "Iteration: 560/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0308\n",
      "Iteration: 561/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0307\n",
      "Iteration: 562/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0306\n",
      "Iteration: 563/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0304\n",
      "Iteration: 564/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0303\n",
      "Iteration: 565/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0302\n",
      "Iteration: 566/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0301\n",
      "Iteration: 567/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0300\n",
      "Iteration: 568/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0298\n",
      "Iteration: 569/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0297\n",
      "Iteration: 570/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0295\n",
      "Iteration: 571/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0294\n",
      "Iteration: 572/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0292\n",
      "Iteration: 573/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0290\n",
      "Iteration: 574/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0287\n",
      "Iteration: 575/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0284\n",
      "Iteration: 576/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0280\n",
      "Iteration: 577/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0275\n",
      "Iteration: 578/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0269\n",
      "Iteration: 579/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0262\n",
      "Iteration: 580/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0255\n",
      "Iteration: 581/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0250\n",
      "Iteration: 582/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0250\n",
      "Iteration: 583/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0253\n",
      "Iteration: 584/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0266\n",
      "Iteration: 585/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0254\n",
      "Iteration: 586/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0342\n",
      "Iteration: 587/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0183\n",
      "Iteration: 588/1000 \t Train Loss: 0.0045 \t Test Loss: 0.1149\n",
      "Iteration: 589/1000 \t Train Loss: 0.0074 \t Test Loss: 0.1206\n",
      "Iteration: 590/1000 \t Train Loss: 0.0127 \t Test Loss: 0.2970\n",
      "Iteration: 591/1000 \t Train Loss: 0.0166 \t Test Loss: 0.0251\n",
      "Iteration: 592/1000 \t Train Loss: 0.0159 \t Test Loss: 0.0523\n",
      "Iteration: 593/1000 \t Train Loss: 0.0082 \t Test Loss: 0.2420\n",
      "Iteration: 594/1000 \t Train Loss: 0.0113 \t Test Loss: 0.0314\n",
      "Iteration: 595/1000 \t Train Loss: 0.0106 \t Test Loss: 0.1052\n",
      "Iteration: 596/1000 \t Train Loss: 0.0066 \t Test Loss: 0.0930\n",
      "Iteration: 597/1000 \t Train Loss: 0.0060 \t Test Loss: 0.0238\n",
      "Iteration: 598/1000 \t Train Loss: 0.0134 \t Test Loss: 0.2923\n",
      "Iteration: 599/1000 \t Train Loss: 0.0165 \t Test Loss: 0.0502\n",
      "Iteration: 600/1000 \t Train Loss: 0.0069 \t Test Loss: 0.0214\n",
      "Iteration: 601/1000 \t Train Loss: 0.0127 \t Test Loss: 0.1713\n",
      "Iteration: 602/1000 \t Train Loss: 0.0083 \t Test Loss: 0.1452\n",
      "Iteration: 603/1000 \t Train Loss: 0.0082 \t Test Loss: 0.0140\n",
      "Iteration: 604/1000 \t Train Loss: 0.0085 \t Test Loss: 0.0163\n",
      "Iteration: 605/1000 \t Train Loss: 0.0062 \t Test Loss: 0.1535\n",
      "Iteration: 606/1000 \t Train Loss: 0.0074 \t Test Loss: 0.1404\n",
      "Iteration: 607/1000 \t Train Loss: 0.0062 \t Test Loss: 0.0215\n",
      "Iteration: 608/1000 \t Train Loss: 0.0065 \t Test Loss: 0.0204\n",
      "Iteration: 609/1000 \t Train Loss: 0.0063 \t Test Loss: 0.0969\n",
      "Iteration: 610/1000 \t Train Loss: 0.0052 \t Test Loss: 0.1070\n",
      "Iteration: 611/1000 \t Train Loss: 0.0063 \t Test Loss: 0.0278\n",
      "Iteration: 612/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0171\n",
      "Iteration: 613/1000 \t Train Loss: 0.0062 \t Test Loss: 0.0544\n",
      "Iteration: 614/1000 \t Train Loss: 0.0043 \t Test Loss: 0.1144\n",
      "Iteration: 615/1000 \t Train Loss: 0.0057 \t Test Loss: 0.0670\n",
      "Iteration: 616/1000 \t Train Loss: 0.0044 \t Test Loss: 0.0242\n",
      "Iteration: 617/1000 \t Train Loss: 0.0052 \t Test Loss: 0.0304\n",
      "Iteration: 618/1000 \t Train Loss: 0.0046 \t Test Loss: 0.0733\n",
      "Iteration: 619/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0726\n",
      "Iteration: 620/1000 \t Train Loss: 0.0048 \t Test Loss: 0.0292\n",
      "Iteration: 621/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0201\n",
      "Iteration: 622/1000 \t Train Loss: 0.0047 \t Test Loss: 0.0419\n",
      "Iteration: 623/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0705\n",
      "Iteration: 624/1000 \t Train Loss: 0.0045 \t Test Loss: 0.0474\n",
      "Iteration: 625/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0246\n",
      "Iteration: 626/1000 \t Train Loss: 0.0043 \t Test Loss: 0.0286\n",
      "Iteration: 627/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0508\n",
      "Iteration: 628/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0531\n",
      "Iteration: 629/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0322\n",
      "Iteration: 630/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0251\n",
      "Iteration: 631/1000 \t Train Loss: 0.0042 \t Test Loss: 0.0373\n",
      "Iteration: 632/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0547\n",
      "Iteration: 633/1000 \t Train Loss: 0.0041 \t Test Loss: 0.0464\n",
      "Iteration: 634/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0307\n",
      "Iteration: 635/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0313\n",
      "Iteration: 636/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0452\n",
      "Iteration: 637/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0494\n",
      "Iteration: 638/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0365\n",
      "Iteration: 639/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0301\n",
      "Iteration: 640/1000 \t Train Loss: 0.0040 \t Test Loss: 0.0374\n",
      "Iteration: 641/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0478\n",
      "Iteration: 642/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0438\n",
      "Iteration: 643/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0339\n",
      "Iteration: 644/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0328\n",
      "Iteration: 645/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0401\n",
      "Iteration: 646/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0435\n",
      "Iteration: 647/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0368\n",
      "Iteration: 648/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0318\n",
      "Iteration: 649/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0355\n",
      "Iteration: 650/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0424\n",
      "Iteration: 651/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0408\n",
      "Iteration: 652/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0343\n",
      "Iteration: 653/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0335\n",
      "Iteration: 654/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0383\n",
      "Iteration: 655/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0399\n",
      "Iteration: 656/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0356\n",
      "Iteration: 657/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0328\n",
      "Iteration: 658/1000 \t Train Loss: 0.0039 \t Test Loss: 0.0353\n",
      "Iteration: 659/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0389\n",
      "Iteration: 660/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0376\n",
      "Iteration: 661/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0341\n",
      "Iteration: 662/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0340\n",
      "Iteration: 663/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0371\n",
      "Iteration: 664/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0379\n",
      "Iteration: 665/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 666/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0338\n",
      "Iteration: 667/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0361\n",
      "Iteration: 668/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0381\n",
      "Iteration: 669/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0366\n",
      "Iteration: 670/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 671/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 672/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0372\n",
      "Iteration: 673/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0369\n",
      "Iteration: 674/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 675/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 676/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0364\n",
      "Iteration: 677/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0371\n",
      "Iteration: 678/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 679/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 680/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0358\n",
      "Iteration: 681/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0369\n",
      "Iteration: 682/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0362\n",
      "Iteration: 683/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 684/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 685/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0366\n",
      "Iteration: 686/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0365\n",
      "Iteration: 687/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 688/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 689/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0361\n",
      "Iteration: 690/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0363\n",
      "Iteration: 691/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 692/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 693/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 694/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0361\n",
      "Iteration: 695/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 696/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 697/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 698/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0359\n",
      "Iteration: 699/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 700/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 701/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 702/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 703/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0357\n",
      "Iteration: 704/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 705/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 706/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 707/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 708/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 709/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 710/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 711/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0356\n",
      "Iteration: 712/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 713/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 714/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 715/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0355\n",
      "Iteration: 716/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 717/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 718/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 719/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0354\n",
      "Iteration: 720/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 721/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 722/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 723/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0353\n",
      "Iteration: 724/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 725/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 726/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 727/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 728/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 729/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 730/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 731/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0352\n",
      "Iteration: 732/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 733/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 734/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 735/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 736/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0351\n",
      "Iteration: 737/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 738/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 739/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 740/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0350\n",
      "Iteration: 741/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 742/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 743/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 744/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 745/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 746/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 747/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 748/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0349\n",
      "Iteration: 749/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 750/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 751/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 752/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 753/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 754/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 755/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 756/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 757/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 758/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 759/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 760/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 761/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 762/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 763/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 764/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 765/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 766/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 767/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 768/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 769/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 770/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 771/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 772/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 773/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 774/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 775/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 776/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 777/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 778/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 779/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 780/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 781/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 782/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 783/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 784/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 785/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 786/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 787/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 788/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 789/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 790/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 791/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 792/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 793/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 794/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 795/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 796/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 797/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 798/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 799/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 800/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0346\n",
      "Iteration: 801/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 802/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 803/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 804/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 805/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 806/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 807/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 808/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 809/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 810/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 811/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 812/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0347\n",
      "Iteration: 813/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 814/1000 \t Train Loss: 0.0038 \t Test Loss: 0.0348\n",
      "Iteration: 815/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 816/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 817/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 818/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 819/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 820/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 821/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0348\n",
      "Iteration: 822/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 823/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 824/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 825/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 826/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 827/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 828/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0349\n",
      "Iteration: 829/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0350\n",
      "Iteration: 830/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0350\n",
      "Iteration: 831/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0350\n",
      "Iteration: 832/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0350\n",
      "Iteration: 833/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0350\n",
      "Iteration: 834/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0350\n",
      "Iteration: 835/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0351\n",
      "Iteration: 836/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0351\n",
      "Iteration: 837/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0351\n",
      "Iteration: 838/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0351\n",
      "Iteration: 839/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0351\n",
      "Iteration: 840/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0351\n",
      "Iteration: 841/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 842/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 843/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 844/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 845/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 846/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 847/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0352\n",
      "Iteration: 848/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 849/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 850/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 851/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 852/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 853/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 854/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 855/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 856/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 857/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 858/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 859/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 860/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 861/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 862/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 863/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 864/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 865/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 866/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 867/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 868/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 869/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 870/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 871/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 872/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 873/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 874/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 875/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 876/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 877/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 878/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0353\n",
      "Iteration: 879/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 880/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 881/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 882/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 883/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 884/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 885/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 886/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 887/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 888/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 889/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 890/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 891/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 892/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0354\n",
      "Iteration: 893/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 894/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 895/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 896/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 897/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 898/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 899/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 900/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 901/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 902/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 903/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0355\n",
      "Iteration: 904/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 905/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 906/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 907/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 908/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 909/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 910/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 911/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0356\n",
      "Iteration: 912/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 913/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 914/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 915/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 916/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 917/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 918/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 919/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0357\n",
      "Iteration: 920/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0358\n",
      "Iteration: 921/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0358\n",
      "Iteration: 922/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0358\n",
      "Iteration: 923/1000 \t Train Loss: 0.0037 \t Test Loss: 0.0358\n",
      "Iteration: 924/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 925/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 926/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 927/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 928/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 929/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 930/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 931/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 932/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 933/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 934/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 935/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 936/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 937/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 938/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 939/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 940/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 941/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0358\n",
      "Iteration: 942/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0357\n",
      "Iteration: 943/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0357\n",
      "Iteration: 944/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0357\n",
      "Iteration: 945/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0356\n",
      "Iteration: 946/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0355\n",
      "Iteration: 947/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0355\n",
      "Iteration: 948/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0354\n",
      "Iteration: 949/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0353\n",
      "Iteration: 950/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0352\n",
      "Iteration: 951/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0351\n",
      "Iteration: 952/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0350\n",
      "Iteration: 953/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0349\n",
      "Iteration: 954/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0348\n",
      "Iteration: 955/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0347\n",
      "Iteration: 956/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0346\n",
      "Iteration: 957/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0345\n",
      "Iteration: 958/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0344\n",
      "Iteration: 959/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0344\n",
      "Iteration: 960/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0343\n",
      "Iteration: 961/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0342\n",
      "Iteration: 962/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0342\n",
      "Iteration: 963/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0341\n",
      "Iteration: 964/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0341\n",
      "Iteration: 965/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0341\n",
      "Iteration: 966/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0340\n",
      "Iteration: 967/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0340\n",
      "Iteration: 968/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0340\n",
      "Iteration: 969/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0340\n",
      "Iteration: 970/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0339\n",
      "Iteration: 971/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0339\n",
      "Iteration: 972/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0338\n",
      "Iteration: 973/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0337\n",
      "Iteration: 974/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0336\n",
      "Iteration: 975/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0335\n",
      "Iteration: 976/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0334\n",
      "Iteration: 977/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0334\n",
      "Iteration: 978/1000 \t Train Loss: 0.0036 \t Test Loss: 0.0333\n",
      "Iteration: 979/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0333\n",
      "Iteration: 980/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0332\n",
      "Iteration: 981/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0332\n",
      "Iteration: 982/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0333\n",
      "Iteration: 983/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0333\n",
      "Iteration: 984/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0333\n",
      "Iteration: 985/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0333\n",
      "Iteration: 986/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0333\n",
      "Iteration: 987/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0332\n",
      "Iteration: 988/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0332\n",
      "Iteration: 989/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0330\n",
      "Iteration: 990/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0329\n",
      "Iteration: 991/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0326\n",
      "Iteration: 992/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0323\n",
      "Iteration: 993/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0320\n",
      "Iteration: 994/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0319\n",
      "Iteration: 995/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0318\n",
      "Iteration: 996/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0321\n",
      "Iteration: 997/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0315\n",
      "Iteration: 998/1000 \t Train Loss: 0.0034 \t Test Loss: 0.0337\n",
      "Iteration: 999/1000 \t Train Loss: 0.0034 \t Test Loss: 0.0269\n",
      "Iteration: 1000/1000 \t Train Loss: 0.0035 \t Test Loss: 0.0633\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_inputs=train_inputs,\n",
    "    train_targets=train_targets,\n",
    "    test_inputs=test_inputs,\n",
    "    test_targets=test_targets,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss per Iteration')"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgDUlEQVR4nO3deZhU9Z3v8fe3qjebfWmVTcGMUXZQVLwkA8SMQUk0GXMzJpoYrwnxPlknuQpejctEJzj6qCHXZUhCNhMSZ8yYqCSgCYhm3ICgomBYxNCg0qDdrA3dVd/7xzndVDW9VDdd1K+bz+t56umqs37POfjxV79z6hxzd0REJFyJQhcgIiKtU1CLiAROQS0iEjgFtYhI4BTUIiKBU1CLiAROQS1SIGb2qplNK3QdEj4FtbTKzDab2YcLXUdnM7NlZvaF+P00M6vM8/p+Yma3Zg5z99Huviyf65XuQUEt3Z6ZJfO8/KJ8Ll9EQS0dYmalZnaPmW2LX/eYWWk8bqCZPWZm1Wb2rpk9bWaJeNxsM9tqZrvN7HUzO6+F5f/EzB4wsyfiaZ8ys5Mzxp8ej3s3Xs6nmsx7v5ktMrO9wPRWtqMH8HtgsJntiV+DzSxhZnPMbKOZ7TSzh8ysfzzPcDNzM7vKzP4G/Cke/h9m9raZ1ZjZcjMbHQ+fBVwGXBsv/9F4eOO3lTb25zQzqzSzb5nZdjN7y8yu7Oixk65HQS0ddT0wGZgAjAfOBm6Ix30LqAQqgBOA/wu4mZ0GfAU4y917AR8BNreyjsuA7wADgdXAL6AxXJ8AfgkcD1wK3GdmozLm/QxwG9ALeKalFbj7XuACYJu794xf24CvAh8HpgKDgfeAe5vMPhUYGW8HRIF/alzTqoZ63X1+/P7f4uV/rJlSWtufACcCfYAhwFXAvWbWr6Xtku5FQS0ddRnwL+6+3d2rgFuAz8bj6oBBwMnuXufuT3t0U5kUUAqMMrNid9/s7htbWcfj7r7c3Q8QBdm5ZjYM+Ciw2d1/7O717v4X4GHgf2bM+1t3/7O7p929tgPbdzVwvbtXxuu/Gfhkk26Om919r7vvB3D3Be6+O2P68WbWJ8f1tbY/Idqn/xLvz0XAHuC0DmyXdEEKaumowcCbGZ/fjIcB3AFsAJaY2SYzmwPg7huAbxCF2HYz+5WZDaZlWxreuPse4N14HScD58RdK9VmVk0UdCc2N28HnQz8V8by1xL9j+aE5tZhZkkzmxt3lezi0DeFgTmur7X9CbDT3eszPu8Deua4bOniFNTSUduIwqzBSfEw4lblt9z9FOAi4JsNfdHu/kt3/0A8rwO3t7KOYQ1vzKwn0D9exxbgKXfvm/Hq6e7/O2Pe9twWsrlptwAXNFlHmbtvbWG+zwAXAx8m6qIY3lB6jvW0uD9FFNSSi2IzK8t4FQELgRvMrMLMBgI3Ag8CmNlHzezvzMyAGqKWaNrMTjOzD8UnyWqB/UC6lfVeaGYfMLMSor7q59x9C/AY8H4z+6yZFcevs8xsZAe37x1gQJNuigeA2xpOYMbbeXEry+gFHAB2AuXAvzazjlNamb/F/SmioJZcLCIK1YbXzcCtwArgZeAVopNnDdcJnwo8SdSP+ixwn7svJeqfngvsAN4mOul2XSvr/SVwE1GXx5nA5RC12IHziU4ibouXdXu8/HZz93VEQbkp7uoYDHwP+B1R981u4DngnFYW8zOi7oqtwGvx9Jl+RNQ3X21mjzQzf2v7U45xpgcHSIjM7CdApbvf0Na0It2dWtQiIoFTUIuIBE5dHyIigVOLWkQkcHm5mczAgQN9+PDh+Vi0iEi3tHLlyh3uXtHcuLwE9fDhw1mxYkU+Fi0i0i2Z2ZstjVPXh4hI4BTUIiKBU1CLiAROT6YQkXapq6ujsrKS2tqO3D1WysrKGDp0KMXFxTnPo6AWkXaprKykV69eDB8+nOi+W5Ird2fnzp1UVlYyYsSInOdT14eItEttbS0DBgxQSHeAmTFgwIB2fxtRUItIuymkO64j+y6soH7q32DDk4WuQkQkKGEF9TP3wMalha5CRAJVXV3Nfffd16F5L7zwQqqrq3Oe/uabb+bOO+/s0Lo6W1hBnSyGVF2hqxCRQLUW1PX19c0Ob7Bo0SL69u2bh6ryL7CgLoHUwUJXISKBmjNnDhs3bmTChAlcc801LFu2jA9+8INcdNFFjBo1CoCPf/zjnHnmmYwePZr58+c3zjt8+HB27NjB5s2bGTlyJF/84hcZPXo0559/Pvv37291vatXr2by5MmMGzeOT3ziE7z33nsAzJs3j1GjRjFu3DguvfRSAJ566ikmTJjAhAkTmDhxIrt37z7i7Q7r8rxkiVrUIl3ILY++ymvbdnXqMkcN7s1NHxvd7Li5c+eyZs0aVq9eDcCyZctYtWoVa9asabzcbcGCBfTv35/9+/dz1llncckllzBgwICs5axfv56FCxfygx/8gE996lM8/PDDXH755S3W9LnPfY7vf//7TJ06lRtvvJFbbrmFe+65h7lz5/LGG29QWlra2K1y5513cu+99zJlyhT27NlDWVnZEe+TwFrURWpRi0i7nH322VnXJM+bN4/x48czefJktmzZwvr16w+bZ8SIEUyYMAGAM888k82bN7e4/JqaGqqrq5k6dSoAV1xxBcuXLwdg3LhxXHbZZTz44IMUFUXt3ilTpvDNb36TefPmUV1d3Tj8SATYolZQi3QVLbV8j6YePXo0vl+2bBlPPvkkzz77LOXl5UybNq3Za5ZLSw89BzmZTLbZ9dGSxx9/nOXLl/Poo49y22238corrzBnzhxmzpzJokWLmDJlCosXL+b000/v0PIbBNaiLoF06ycEROTY1atXr1b7fGtqaujXrx/l5eWsW7eO555r+jD49uvTpw/9+vXj6aefBuDnP/85U6dOJZ1Os2XLFqZPn87tt99OTU0Ne/bsYePGjYwdO5bZs2dz1llnsW7duiOuIbAWdbFa1CLSogEDBjBlyhTGjBnDBRdcwMyZM7PGz5gxgwceeICRI0dy2mmnMXny5E5Z709/+lOuvvpq9u3bxymnnMKPf/xjUqkUl19+OTU1Nbg7X/va1+jbty/f/va3Wbp0KYlEgtGjR3PBBRcc8fpzemaimW0GdgMpoN7dJ7U2/aRJk7xDDw740flQfBx87rftn1dEjoq1a9cycuTIQpfRpTW3D81sZUvZ2p4W9XR333EkxbVJV32IiBwmrD7qhK76EBFpKtegdmCJma00s1nNTWBms8xshZmtqKqq6lg1uupDROQwuQb1B9z9DOAC4Mtm9vdNJ3D3+e4+yd0nVVQ0+yDdtiWLIaWrPkREMuUU1O6+Nf67Hfgv4Oy8VJMsgdSBvCxaRKSrajOozayHmfVqeA+cD6zJTzVFkE7lZdEiIl1VLi3qE4BnzOwl4AXgcXf/Q36qSSqoRaRFR3KbU4B77rmHffv2NTtu2rRpdOiy4qOgzaB2903uPj5+jXb32/JXTVK/TBSRFuUzqEMW3uV5rha1iDSv6W1OAe644w7OOussxo0bx0033QTA3r17mTlzJuPHj2fMmDH8+te/Zt68eWzbto3p06czffr0VtezcOFCxo4dy5gxY5g9ezYAqVSKz3/+84wZM4axY8dy9913A83f6rSzhfUT8kSRWtQiXcnv58Dbr3TuMk8cCxfMbXZU09ucLlmyhPXr1/PCCy/g7lx00UUsX76cqqoqBg8ezOOPPw5E9wDp06cPd911F0uXLmXgwIEtrn7btm3Mnj2blStX0q9fP84//3weeeQRhg0bxtatW1mzJjpF13Bb0+ZuddrZwmpRm7o+RCR3S5YsYcmSJUycOJEzzjiDdevWsX79esaOHcsTTzzB7Nmzefrpp+nTp0/Oy3zxxReZNm0aFRUVFBUVcdlll7F8+XJOOeUUNm3axFe/+lX+8Ic/0Lt3b6D5W512tgBb1OlCVyEiuWqh5Xu0uDvXXXcdX/rSlw4bt2rVKhYtWsQNN9zAeeedx4033nhE6+rXrx8vvfQSixcv5oEHHuChhx5iwYIFzd7qtLMDO6wWtU4mikgrmt7m9CMf+QgLFixgz549AGzdupXt27ezbds2ysvLufzyy7nmmmtYtWpVs/M35+yzz+app55ix44dpFIpFi5cyNSpU9mxYwfpdJpLLrmEW2+9lVWrVrV4q9POFliLWkEtIi1repvTO+64g7Vr13LuuecC0LNnTx588EE2bNjANddcQyKRoLi4mPvvvx+AWbNmMWPGDAYPHszSpUubXcegQYOYO3cu06dPx92ZOXMmF198MS+99BJXXnkl6fhb/3e/+90Wb3Xa2XK6zWl7dfg2p3+6FZbfCTdXd3pNItI5dJvTI9fe25wG1vVRBLj6qUVEMgQW1Mnor66lFhFpFFZQWxzU6qcWCVo+ukyPFR3Zd2EFdSI+t6n7fYgEq6ysjJ07dyqsO8Dd2blzJ2VlZe2aL7CrPhqCWi1qkVANHTqUyspKOvyAkGNcWVkZQ4cObdc8gQV1Q9eHWtQioSouLmbEiBGFLuOYEljXh/qoRUSaCiyo4wa+rvoQEWkUZlCrRS0i0iisoNbleSIihwkrqBtb1PploohIg8CCWi1qEZGmFNQiIoELLKh1MlFEpKkwg1qX54mINAosqPXLRBGRpsIKal2eJyJymLCCWnfPExE5TKBBrRa1iEiDwIJafdQiIk0FGtRqUYuINMg5qM0saWZ/MbPH8leNLs8TEWmqPS3qrwNr81UIoD5qEZFm5BTUZjYUmAn8MK/V6PI8EZHD5Nqivge4Fsjvbe109zwRkcO0GdRm9lFgu7uvbGO6WWa2wsxWdPihlzqZKCJymFxa1FOAi8xsM/Ar4ENm9mDTidx9vrtPcvdJFRUVHaxGQS0i0lSbQe3u17n7UHcfDlwK/MndL89PNTqZKCLSVGDXUevyPBGRporaM7G7LwOW5aUS0L0+RESaEVaL2uJy1PUhItIorKBWH7WIyGECC2rdlElEpKmwgrrhl4k6mSgi0iisoG5sUeuXiSIiDcIK6oaTiWpRi4g0CiyoLQpr9VGLiDQKK6gh6qdWi1pEpFF4QZ1IqkUtIpIhvKC2JLhOJoqINAgvqNWiFhHJEl5QW0J91CIiGcILarWoRUSyhBfUuupDRCRLeEGtFrWISJbwglpXfYiIZAkvqBP6ZaKISKbwglp91CIiWcILavVRi4hkCS+o1aIWEckSXlCrRS0ikiW8oNZVHyIiWcILal31ISKSJbygtqSeQi4ikiG8oE4U6WSiiEiGAINaJxNFRDKFF9Q6mSgikiW8oNbJRBGRLOEFtX7wIiKSpc2gNrMyM3vBzF4ys1fN7Jb8VqQ+ahGRTEU5THMA+JC77zGzYuAZM/u9uz+Xl4rUohYRydJmULu7A3vij8Xxy/NWUSIJaZ1MFBFpkFMftZklzWw1sB14wt2fb2aaWWa2wsxWVFVVdbwiPdxWRCRLTkHt7il3nwAMBc42szHNTDPf3Se5+6SKioojqEh91CIimdp11Ye7VwNLgRl5qQbURy0i0kQuV31UmFnf+P1xwD8A6/JXkVrUIiKZcrnqYxDwUzNLEgX7Q+7+WN4qUotaRCRLLld9vAxMPAq1RHTVh4hIlgB/mairPkREMoUX1OqjFhHJEl5Qq49aRCRLeEGtFrWISJbwglr3oxYRyRJeUKtFLSKSJbyg1lUfIiJZwgtqtahFRLKEF9S66kNEJEt4QZ2ITyZ6/m55LSLSlYQX1JaM/urKDxERIMSgTsQlqZ9aRAQIMagbW9QKahERCDGoE3FQq0UtIgKEGNRqUYuIZAkvqNWiFhHJEmBQx88yUFCLiAAhBrXFJanrQ0QECDGo1fUhIpIlvKDWyUQRkSzhBbVa1CIiWcILav2EXEQkS3hBrRa1iEiW8IJaV32IiGQJL6jVohYRyRJeUOuqDxGRLOEFtVrUIiJZwgtqXfUhIpKlzaA2s2FmttTMXjOzV83s6/mtSA8OEBHJVJTDNPXAt9x9lZn1Alaa2RPu/lpeKlIftYhIljZb1O7+lruvit/vBtYCQ/JXkfqoRUQytauP2syGAxOB55sZN8vMVpjZiqqqqo5XpBa1iEiWnIPazHoCDwPfcPddTce7+3x3n+TukyoqKo6gIrWoRUQy5RTUZlZMFNK/cPff5LUiXfUhIpIll6s+DPgRsNbd78p/RbrqQ0QkUy4t6inAZ4EPmdnq+HVh3ipSH7WISJY2L89z92cAOwq1RNRHLSKSJeBfJiqoRUQgxKBWi1pEJEt4Qa2rPkREsoQX1I0t6vrC1iEiEogAgzo+v6mgFhEBQgzqZHH0N1VX2DpERAIRXlA3tqh1MlFEBIIOarWoRUQgxKBW14eISJbwgjoRB7Va1CIiQJBBrT5qEZFMAQZ1Aiyhrg8RkVh4QQ1R94e6PkREgFCDOlkMKf3gRUQEQg3qRJFa1CIisYCDWi1qEREINaiTxTqZKCISCzOoE8VqUYuIxMIM6mSRWtQiIrEwg1qX54mINAo0qIv0y0QRkViYQa2uDxGRRmEGtbo+REQahRnUujxPRKRRmEGtH7yIiDRSUIuIBC7MoFbXh4hIozCDWr9MFBFpVFToAhq4O1W7D2BmVOjyPBGRRm22qM1sgZltN7M1+Swk7fCB25fyw2c2qY9aRCRDLl0fPwFm5LkOkgljWP/j2Lxjr66jFhHJ0GZQu/ty4N2jUAvDB/TgzZ374l8mqkUtIgKdeDLRzGaZ2QozW1FVVdWhZQzuexxv1dSqRS0ikqHTgtrd57v7JHefVFFR0aFlnNC7lJr9ddSbTiaKiDQI6vK843uVAbC/Ht09T0QkFlRQ9+tRAkBtOqGuDxGRWC6X5y0EngVOM7NKM7sqX8X0Kosu665NJyF1ENzztSoRkS6jzR+8uPunj0YhAL3LigHY76Xg6aifuqjkaK1eRCRIQXV9NLSo93kU2NTtK2A1IiJhCCqoex8XBfSedNyKrttfwGpERMIQVFD3LI1a1HtScY9MvYJaRCSooE4mjF6lRexONXR9KKhFRIIKaoi6P3bVxy3qutrCFiMiEoDggrpXWRHVdcnog04mioiEF9S9y4p5ry7u+ji4t7DFiIgEILig7lVWxFt15dGH/Uflpn0iIkELLqh7H1fM1oM9og97dxS2GBGRAIQX1GVFvF0bn0xc9l3dRU9EjnnBBXWvsmJ2H0jhx4+KTiZuXVXokkRECiq4oO59XBGptLP/Yw9EA3ZvK2xBIiIFFlxQD+xZCkAV/aMBu94qYDUiIoUXXFCf2Cd6eMDWA2VgSdjbscd6iYh0F8EF9eA+xwFQWV0LJT31oxcROeYFF9TD+pdTVpxg3Vu7oaQcDu4pdEkiIgUVXFAnE8bYIX14btNOKOkBB9WiFpFjW3BBDXDeyBN47a1dHEyWR79O/P0c2KO+ahE5NgUZ1H9/agUAu1IlsGkZPH8/LPvXwhYlIlIgQQb1+47vQcJgV7r00MBEm493FAnHpqfgd1+NHtD853lQU1noiqQLCzKoS4uSnNS//NBd9ABKexWuIJH2+tlFsOpn8O4meOLb8KvLCl2RdGFBBjXA+yp6UnUgI6j1tBfpipbeFv2trS5oGdK1BRvUpw/qxTu1GeXtry5YLSIdtubhQlcg3UCwQT12SF/2eEYftVok0pXpLpByBIIN6vHD+rDXyw4NqK0pXDHd2eLr4bFvFrqK7m/XVvjNLJ1UlA4JNqhP7F1G8XEZJxDf/DPc9z9g/3uFK6o7evb/wYofFbqKY8PLv4a7Rxe6CumCgg1qM2PwoEHZA7e/ClV/LUxB3d3qhbpTYWep3dX6+IWfgVv6Q+XKo1OPdHnBBjVA3/EfY0u6gr0Dxx8auHd79HfnRlh0bXh9f4uvh5v7wAs/gL89B6n6QlfUsvqDh94/cjXcdTr8+Xuw7S+Fq6k7qH6z9fGvPw6egp9dDG8s18Mxuou9O/N2dZq5e9sTmc0AvgckgR+6+9zWpp80aZKvWLHiiIur3neQid95grsnvsPHX/vnQyNG/yO8+ptDn4ecCRhsjdd52kw46RzwNLz7BhQfF/1gprx/NCxZAsXl0csMUgej+S0RfbZEM59pY3y8/uV3HL4hw86Jaux5PCRLoX4/9DwB0qmorqJSSBZHdSWLoaQXpA5E87pDaU9Ip6P1JBLxepOQSGa8j/8mi6NlNr6SGdPG0zV4/P/Aiz9ofuf3Ggx9hsDxI6NaS3tHN8kq7R0tC4vuxZIsgfIBh9aHR9vVsL+Ky6Lhloj2vafjfWfZ+7DZ/d3Kvm52nOX07yrvNv4Jfv6J9s3z/hnRbwXKB0T7u6w3DHx/9O+lz5BoPzfsxwbpVHwsOLQ/0qmM/dn0b6LJPsp43zjc2vjcjFD2e6EtugZefgiufSP7v7McmdlKd5/U7Li2gtrMksBfgX8AKoEXgU+7+2stzdNZQQ3wyfv/m9Vb3uP6Uyu58s1rO2WZxzyLAz4d2LeRTtE0xJsJ9Mb3tDKu6TKa+R9LshiKyuJXadQgKCqF135bsK3vOjoa/C2M68g8nb2u+lo4cRxc/XQry2ytlJaDOpffZZ8NbHD3TfHCfgVcDLQY1J3p7n+awB2LX+c7L8O+5EV8ueh3APw1PYQ/p8cwILGX9ye2sI5TeCpxDm8kTmI0mzjBd/C2VfBG4mTes96Us58iUuxPlJPGSAD92MVBiqmjGCONAQnSEP81AG9uuGe8ouFJT3F2ejVfqFsIwAGKWZ0Yw0DfSR3FpDH+lhjCXsqpsV4coJS9Vk6dF7HLelJPEidBiiRGGidBXXx4ytlPigSGk8BJkCZJGiNNwtMkSDUOi14pijxFEfVYPCwRvxrel3CQz/Bo1r5eYlN4MzGU9/tmhnsltVbKQUrYbhXUWgnv2PEcsFIOWCm1lGJ4vO+cJCmSpKmjiBRJov/9G6UciNft8Z5LxGOi/dq4D90bty9z/2YeC/PsY5A1Lmt58cv98M8Z6yHjGGYtww+NTzTWcfiwpKco4SCl7KWEakr9ICXUMSLen+vTQ+hp+xlk7zb7b/tNP5EBVsNBitlND3ZaP961vuxN9CRNkj3Wkx7s4yDFHKSEBGnqrJhUvA9rOY4EKQzHSZAgRZpkvH+z92Hm+4bPTd8fih9vYfjhrGlDr3HilhuA1uq4VtbV4nzeZLrclteRGltb3vFFVbxYO53Zra6zY3JpUX8SmOHuX4g/fxY4x92/0mS6WcAsgJNOOunMN99so5+unfYdrGftW7vYuecg7+w+QM2+gxyoT3OgPs3B+jSptJNyJ512Uulom5yo5yB6743HJRp+aJq2vrhZG1/t2vzi18YE1sYEbX2zbLv+VsY59OtRwr66FHUppz6VJu2Qdifd5N9GDr1kh8ll/+ZSZ9Z07Vh/W8euI8vMZeJS6hg1rILdB+r52859lHot7yt5lx2lw0mk9nPQE9RRTH0qTcqJ/v2mnfp09G+4Pt3+nd2Bw9OpculGzXsNBV5/77IivvuP4zo075G2qHPi7vOB+RB1fXTWchuUlxRx5sn9O3uxIiLBy6XHeyswLOPz0HiYiIgcBbkE9YvAqWY2wsxKgEuB3+W3LBERadBm14e715vZV4DFRJfnLXD3V/NemYiIADn2Ubv7ImBRnmsREZFmBP3LRBERUVCLiARPQS0iEjgFtYhI4HK6KVO7F2pWBXT0p4kDgR2dWE5XoG0+Nmibu78j2d6T3b2iuRF5CeojYWYrWvoZZXelbT42aJu7v3xtr7o+REQCp6AWEQlciEE9v9AFFIC2+digbe7+8rK9wfVRi4hIthBb1CIikkFBLSISuGCC2sxmmNnrZrbBzOYUup7OYmbDzGypmb1mZq+a2dfj4f3N7AkzWx//7RcPNzObF++Hl83sjMJuQceZWdLM/mJmj8WfR5jZ8/G2/Tq+bS5mVhp/3hCPH17QwjvIzPqa2X+a2TozW2tm53b342xm/xz/u15jZgvNrKy7HWczW2Bm281sTcawdh9XM7sinn69mV3RnhqCCOr4Abr3AhcAo4BPm9mowlbVaeqBb7n7KGAy8OV42+YAf3T3U4E/xp8h2genxq9ZwP1Hv+RO83Vgbcbn24G73f3vgPeAq+LhVwHvxcPvjqfrir4H/MHdTwfGE217tz3OZjYE+Bowyd3HEN0G+VK633H+CTCjybB2HVcz6w/cBJxD9BzamxrCPSfuXvAXcC6wOOPzdcB1ha4rT9v6W6Inur8ODIqHDQJej9//O9FT3humb5yuK72IngT0R+BDwGNETxrcARQ1PeZE9zo/N35fFE9nhd6Gdm5vH+CNpnV35+MMDAG2AP3j4/YY8JHueJyB4cCajh5X4NPAv2cMz5qurVcQLWoOHfAGlfGwbiX+qjcReB44wd3fike9DZwQv+8u++Ie4FqIH9MOA4Bqd6+PP2duV+M2x+Nr4um7khFAFfDjuLvnh2bWg258nN19K3An8DfgLaLjtpLufZwbtPe4HtHxDiWouz0z6wk8DHzD3XdljvPof7Hd5jpJM/sosN3dVxa6lqOoCDgDuN/dJwJ7OfR1GOiWx7kfcDHR/6QGAz04vIug2zsaxzWUoO7WD9A1s2KikP6Fu/8mHvyOmQ2Kxw8CtsfDu8O+mAJcZGabgV8RdX98D+hrZg1PFcrcrsZtjsf3AXYezYI7QSVQ6e7Px5//kyi4u/Nx/jDwhrtXuXsd8BuiY9+dj3OD9h7XIzreoQR1t32ArpkZ8CNgrbvflTHqd0DDmd8riPquG4Z/Lj57PBmoyfiK1SW4+3XuPtTdhxMdyz+5+2XAUuCT8WRNt7lhX3wynr5LtTzd/W1gi5mdFg86D3iNbnycibo8JptZefzvvGGbu+1xztDe47oYON/M+sXfRM6Ph+Wm0J30GZ3rFwJ/BTYC1xe6nk7crg8QfS16GVgdvy4k6pv7I7AeeBLoH09vRFfAbAReITqjXvDtOILtnwY8Fr8/BXgB2AD8B1AaDy+LP2+Ix59S6Lo7uK0TgBXxsX4E6NfdjzNwC7AOWAP8HCjtbscZWEjUB19H9M3pqo4cV+B/xdu+AbiyPTXoJ+QiIoELpetDRERaoKAWEQmcglpEJHAKahGRwCmoRUQCp6AWEQmcglpEJHD/H3Dm4AEUxFXoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(test_losses, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss per Iteration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18094044df0>]"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_targets[0].cpu())\n",
    "plt.plot(model(test_inputs[0].view(1, T, -1)).view(-1).cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to plot Horizon\n"
     ]
    }
   ],
   "source": [
    "def plot_horizon(): \n",
    "    horizon_predictions = np.array([])\n",
    "    horizon_inputs = test_inputs[0]\n",
    "    \n",
    "    while horizon_predictions.shape[0] < test_targets.shape[0]:\n",
    "        prediction = model(horizon_inputs.view(1, T, 1)).view(-1)\n",
    "        \n",
    "        horizon_predictions = np.concatenate((horizon_predictions, prediction.cpu().detach()), axis=0)\n",
    "        \n",
    "        horizon_inputs = torch.concat((horizon_inputs[-(T-PredictionSteps):].view(-1), prediction[-T:]))\n",
    "    \n",
    "    plt.plot(test_targets.cpu()[:, 0])  \n",
    "    plt.plot(horizon_predictions)\n",
    "\n",
    "plot_horizon() if X.shape == Y.shape else print('Unable to plot Horizon')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "862aaaa3bfdfe0cb3b02e12d68ad12f84f678ba56b93b757d8f1a9844b9ba170"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
